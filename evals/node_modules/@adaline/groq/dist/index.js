'use strict';

var provider = require('@adaline/provider');
var zod = require('zod');
var openai = require('@adaline/openai');

var t=zod.z.object({modelName:zod.z.string().min(1),apiKey:zod.z.string().min(1)}),a=class extends openai.BaseChatModel{constructor(n,r){let l=t.parse(r),m=d.baseUrl;super(n,{modelName:l.modelName,apiKey:l.apiKey,baseUrl:m,completeChatUrl:`${m}/chat/completions`,streamChatUrl:`${m}/chat/completions`});this.version="v1";this.modelSchema=n,this.modelName=l.modelName,this.groqApiKey=l.apiKey;}transformMessages(n){let r=super.transformMessages(n);return r.messages.forEach(l=>{l.role==="system"?typeof l.content!="string"&&(l.content=l.content.map(m=>m.text).join(`
`)):l.role==="assistant"&&l.content&&typeof l.content!="string"&&(l.content=l.content.map(m=>m.text).join(`
`));}),r}};var S=provider.RangeConfigItem({param:"temperature",title:provider.CHAT_CONFIG.TEMPERATURE.title,description:provider.CHAT_CONFIG.TEMPERATURE.description,min:0,max:2,step:.01,default:1}),G=s=>provider.RangeConfigItem({param:"max_tokens",title:provider.CHAT_CONFIG.MAX_TOKENS.title,description:provider.CHAT_CONFIG.MAX_TOKENS.description,min:0,max:s,step:1,default:0}),A=provider.MultiStringConfigItem({param:"stop",title:provider.CHAT_CONFIG.STOP(4).title,description:provider.CHAT_CONFIG.STOP(4).description,max:4}),E=provider.RangeConfigItem({param:"top_p",title:provider.CHAT_CONFIG.TOP_P.title,description:provider.CHAT_CONFIG.TOP_P.description,min:0,max:1,step:.01,default:1}),R=provider.RangeConfigItem({param:"frequency_penalty",title:provider.CHAT_CONFIG.FREQUENCY_PENALTY.title,description:provider.CHAT_CONFIG.FREQUENCY_PENALTY.description,min:-2,max:2,step:.01,default:0}),q=provider.RangeConfigItem({param:"presence_penalty",title:provider.CHAT_CONFIG.PRESENCE_PENALTY.title,description:provider.CHAT_CONFIG.PRESENCE_PENALTY.description,min:-2,max:2,step:.01,default:0}),B=provider.RangeConfigItem({param:"seed",title:provider.CHAT_CONFIG.SEED.title,description:provider.CHAT_CONFIG.SEED.description,min:0,max:1e6,step:1,default:0}),k=provider.SelectStringConfigItem({param:"response_format",title:provider.CHAT_CONFIG.RESPONSE_FORMAT.title,description:provider.CHAT_CONFIG.RESPONSE_FORMAT.description,default:"text",choices:["text","json_object"]}),z=provider.SelectStringConfigItem({param:"tool_choice",title:"Tool choice",description:"Controls which (if any) tool is called by the model.     'none' means the model will not call a function.     'auto' means the model can pick between generating a message or calling a tool.",default:"auto",choices:["auto","required","none"]});var de=s=>zod.z.object({temperature:S.schema,maxTokens:G(s).schema,stop:A.schema,topP:E.schema,frequencyPenalty:R.schema,presencePenalty:q.schema,seed:B.schema.transform(o=>o===0?void 0:o),responseFormat:k.schema,toolChoice:z.schema}),_e=s=>({temperature:S.def,maxTokens:G(s).def,stop:A.def,topP:E.def,frequencyPenalty:R.def,presencePenalty:q.def,seed:B.def,responseFormat:k.def,toolChoice:z.def});var e={base:s=>({def:_e(s),schema:de(s)})};var v="gemma-7b-it",ve="Gemma is a family of lightweight, state-of-the-art open models from Google,   built from the same research and technology used to create the Gemini models.",P=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:v,description:ve,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),ce=t,_=class extends a{constructor(o){super(P,o);}};var U="gemma2-9b-it",Fe="Gemma is a family of lightweight, state-of-the-art open models from Google,   built from the same research and technology used to create the Gemini models.",V=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:U,description:Fe,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),he=t,c=class extends a{constructor(o){super(V,o);}};var w="llama-3.1-70b-versatile",Qe="The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and   outperform many of the available open source and closed chat models on common industry benchmarks.",N=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:w,description:Qe,maxInputTokens:128e3,maxOutputTokens:8192,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(8192).def,schema:e.base(8192).schema}}),Me=t,h=class extends a{constructor(o){super(N,o);}};var F="llama-3.1-8b-instant",eo="The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and   outperform many of the available open source and closed chat models on common industry benchmarks.",D=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:F,description:eo,maxInputTokens:128e3,maxOutputTokens:8192,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(8192).def,schema:e.base(8192).schema}}),fe=t,M=class extends a{constructor(o){super(D,o);}};var j="llama-3.2-11b-vision-preview",lo="The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning,   and answering general questions about an image.   The models outperform many of the available open source and closed multimodal models on common industry benchmarks.",K=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelModalitiesEnum).parse({name:j,description:lo,maxInputTokens:128e3,maxOutputTokens:8192,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelModalities,config:{def:e.base(8192).def,schema:e.base(8192).schema}}),be=t,f=class extends a{constructor(o){super(K,o);}};var Y="llama-3.2-1b-preview",co="The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and   summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.",$=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:Y,description:co,maxInputTokens:128e3,maxOutputTokens:8192,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(8192).def,schema:e.base(8192).schema}}),ue=t,b=class extends a{constructor(o){super($,o);}};var Q="llama-3.2-3b-preview",To="The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and   summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.",X=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:Q,description:To,maxInputTokens:128e3,maxOutputTokens:8192,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(8192).def,schema:e.base(8192).schema}}),Te=t,u=class extends a{constructor(o){super(X,o);}};var Z="llama-3.2-90b-vision-preview",go="The Llama 3.2-90B Vision instruction-tuned models are optimized for advanced visual recognition,   complex image reasoning, detailed captioning, and answering intricate questions about images.   These models achieve state-of-the-art results on multiple industry benchmarks for multimodal tasks.",H=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelModalitiesEnum).parse({name:Z,description:go,maxInputTokens:131072,maxOutputTokens:8192,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelModalities,config:{def:e.base(8192).def,schema:e.base(8192).schema}}),Oe=t,T=class extends a{constructor(o){super(H,o);}};var J="llama3-groq-70b-8192-tool-use-preview",Ro="This is the 70B parameter version of the Llama 3 Groq Tool Use model,   specifically designed for advanced tool use and function calling tasks.",W=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:J,description:Ro,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),ye=t,O=class extends a{constructor(o){super(W,o);}};var ee="llama3-70b-8192",Po="The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of   the available open source chat models on common industry benchmarks.",oe=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:ee,description:Po,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),Ce=t,y=class extends a{constructor(o){super(oe,o);}};var ae="llama3-groq-8b-8192-tool-use-preview",Do="This is the 8B parameter version of the Llama 3 Groq Tool Use model,   specifically designed for advanced tool use and function calling tasks.",te=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:ae,description:Do,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),Le=t,C=class extends a{constructor(o){super(te,o);}};var se="llama3-8b-8192",Xo="The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of   the available open source chat models on common industry benchmarks.",ne=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:se,description:Xo,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),xe=t,L=class extends a{constructor(o){super(ne,o);}};var le="llama-guard-3-8b",oa="Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification.",ie=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:le,description:oa,maxInputTokens:8192,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),ge=t,x=class extends a{constructor(o){super(ie,o);}};var me="mixtral-8x7b-32768",ia="The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",re=provider.ChatModelSchema(openai.OpenAIChatModelRoles,openai.OpenAIChatModelTextToolModalitiesEnum).parse({name:me,description:ia,maxInputTokens:32768,maxOutputTokens:4096,roles:openai.OpenAIChatModelRolesMap,modalities:openai.OpenAIChatModelTextToolModalities,config:{def:e.base(4096).def,schema:e.base(4096).schema}}),Ie=t,g=class extends a{constructor(o){super(re,o);}};var ma="groq",d=class{constructor(){this.version="v1";this.name=ma;this.chatModelFactories={[v]:{model:_,modelOptions:ce,modelSchema:P},[U]:{model:c,modelOptions:he,modelSchema:V},[le]:{model:x,modelOptions:ge,modelSchema:ie},[se]:{model:L,modelOptions:xe,modelSchema:ne},[ee]:{model:y,modelOptions:Ce,modelSchema:oe},[F]:{model:M,modelOptions:fe,modelSchema:D},[ae]:{model:C,modelOptions:Le,modelSchema:te},[w]:{model:h,modelOptions:Me,modelSchema:N},[J]:{model:O,modelOptions:ye,modelSchema:W},[j]:{model:f,modelOptions:be,modelSchema:K},[Z]:{model:T,modelOptions:Oe,modelSchema:H},[Q]:{model:u,modelOptions:Te,modelSchema:X},[Y]:{model:b,modelOptions:ue,modelSchema:$},[me]:{model:g,modelOptions:Ie,modelSchema:re}};this.embeddingModelFactories={};}chatModelLiterals(){return Object.keys(this.chatModelFactories)}chatModelSchemas(){return Object.keys(this.chatModelFactories).reduce((o,n)=>(o[n]=this.chatModelFactories[n].modelSchema,o),{})}chatModel(o){let n=o.modelName;if(!(n in this.chatModelFactories))throw new provider.ProviderError({info:`Groq chat model: ${n} not found`,cause:new Error(`Groq chat model: ${n} not found, available chat models: 
          ${this.chatModelLiterals().join(", ")}`)});let r=this.chatModelFactories[n].model,l=this.chatModelFactories[n].modelOptions.parse(o);return new r(l)}embeddingModelLiterals(){return Object.keys(this.embeddingModelFactories)}embeddingModelSchemas(){return Object.keys(this.embeddingModelFactories).reduce((o,n)=>(o[n]=this.embeddingModelFactories[n].modelSchema,o),{})}embeddingModel(o){throw new provider.ProviderError({info:"Groq does not support embedding models yet",cause:new Error("Groq does not support embedding models yet")})}};d.baseUrl="https://api.groq.com/openai/v1";

exports.BaseChatModelGroq = a;
exports.BaseChatModelOptions = t;
exports.Gemma2_9b_IT = c;
exports.Gemma2_9b_ITLiteral = U;
exports.Gemma2_9b_ITOptions = he;
exports.Gemma2_9b_ITSchema = V;
exports.Gemma_7b_IT = _;
exports.Gemma_7b_ITLiteral = v;
exports.Gemma_7b_ITOptions = ce;
exports.Gemma_7b_ITSchema = P;
exports.Groq = d;
exports.LlamaGuard_3_8b = x;
exports.LlamaGuard_3_8bLiteral = le;
exports.LlamaGuard_3_8bOptions = ge;
exports.LlamaGuard_3_8bSchema = ie;
exports.Llama_3_1_70b = h;
exports.Llama_3_1_70bLiteral = w;
exports.Llama_3_1_70bSchema = N;
exports.Llama_3_1_70b_Options = Me;
exports.Llama_3_1_8b = M;
exports.Llama_3_1_8bLiteral = F;
exports.Llama_3_1_8bSchema = D;
exports.Llama_3_1_8b_Options = fe;
exports.Llama_3_2_11b_Vision = f;
exports.Llama_3_2_11b_VisionLiteral = j;
exports.Llama_3_2_11b_VisionOptions = be;
exports.Llama_3_2_11b_VisionSchema = K;
exports.Llama_3_2_1b = b;
exports.Llama_3_2_1bLiteral = Y;
exports.Llama_3_2_1bSchema = $;
exports.Llama_3_2_1b_Options = ue;
exports.Llama_3_2_3b = u;
exports.Llama_3_2_3bLiteral = Q;
exports.Llama_3_2_3bSchema = X;
exports.Llama_3_2_3b_Options = Te;
exports.Llama_3_2_90b_Vision = T;
exports.Llama_3_2_90b_VisionLiteral = Z;
exports.Llama_3_2_90b_VisionOptions = Oe;
exports.Llama_3_2_90b_VisionSchema = H;
exports.Llama_3_70b = y;
exports.Llama_3_70bLiteral = ee;
exports.Llama_3_70bOptions = Ce;
exports.Llama_3_70bSchema = oe;
exports.Llama_3_70b_Tool_Use = O;
exports.Llama_3_70b_Tool_UseLiteral = J;
exports.Llama_3_70b_Tool_UseSchema = W;
exports.Llama_3_70b_Tool_Use_Options = ye;
exports.Llama_3_8b = L;
exports.Llama_3_8bLiteral = se;
exports.Llama_3_8bOptions = xe;
exports.Llama_3_8bSchema = ne;
exports.Llama_3_8b_Tool_Use = C;
exports.Llama_3_8b_Tool_UseLiteral = ae;
exports.Llama_3_8b_Tool_UseSchema = te;
exports.Llama_3_8b_Tool_Use_Options = Le;
exports.Mixtral_8x7b = g;
exports.Mixtral_8x7bLiteral = me;
exports.Mixtral_8x7bOptions = Ie;
exports.Mixtral_8x7bSchema = re;
//# sourceMappingURL=index.js.map
//# sourceMappingURL=index.js.map